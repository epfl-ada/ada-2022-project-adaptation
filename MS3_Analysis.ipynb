{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "e2af5202-cd4d-4fe2-9e7e-48869ec70229",
         "metadata": {},
         "source": [
            "# Part 0: Initialising\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b347be5c-381b-4203-a676-ae35909a42cc",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Initializing\n",
            "import matplotlib.font_manager as font_manager\n",
            "from matplotlib.lines import Line2D\n",
            "import matplotlib as mpl\n",
            "import matplotlib.ticker as mtick\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import seaborn as sns\n",
            "import os\n",
            "import re\n",
            "from scipy.stats import bootstrap\n",
            "import statsmodels.api as sm\n",
            "import statsmodels.formula.api as smf\n",
            "import requests\n",
            "\n",
            "params = {\n",
            "    \"axes.titlesize\": 14,\n",
            "    \"axes.labelsize\": 12,\n",
            "    \"font.size\": 12,\n",
            "    \"legend.fontsize\": 12,\n",
            "    \"xtick.labelsize\": 12,\n",
            "    \"ytick.labelsize\": 12,\n",
            "    \"text.usetex\": False,\n",
            "}\n",
            "\n",
            "NUM_COLORS = 15\n",
            "cm = plt.get_cmap(\"nipy_spectral\")\n",
            "\n",
            "mpl.rcParams.update(params)\n",
            "\n",
            "import warnings\n",
            "\n",
            "warnings.filterwarnings(\"ignore\")\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "9c4e5555-b4fc-4cd9-acea-f54ff5aef1ad",
         "metadata": {},
         "source": [
            "To start with, we import three datasets that are available from YouNiverse:\n",
            "\n",
            "` ` df_channels_en.tsv.gz ` `\n",
            "\n",
            "` ` df_timeseries_en.tsv.gz ` `\n",
            "\n",
            "` ` yt_metadata_helper.feather ` `\n",
            "\n",
            "To start with, we will filter the channels that have been identified as Gaming, as it is our target category. We will do this by identifying the channel IDs with label ` ` Gaming `  ` in `  ` Category `  ` from `  ` df_channels_en.tsv.gz ` ` .\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "df97ef38",
         "metadata": {},
         "outputs": [],
         "source": [
            "df_channels = pd.read_csv(\"./data/df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
            "df_channels[\"join_date\"] = pd.to_datetime(df_channels[\"join_date\"])\n",
            "\n",
            "df_channels = df_channels[df_channels[\"category_cc\"] == \"Gaming\"]\n",
            "channel_id_gaming = df_channels.channel\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "d5ca7894",
         "metadata": {},
         "source": [
            "` ` channel_id_gaming `  ` contains all channel IDs that are identified as Gaming.  we will then use this list to do initial filtering of both `  ` df_time_series `  ` and `  ` video_metadatas ` ` .\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2c93da3e-46e0-4e9b-98dc-c725a9d981fa",
         "metadata": {},
         "outputs": [],
         "source": [
            "df_time_series = pd.read_csv(\n",
            "    \"./data/df_timeseries_en.tsv.gz\", compression=\"infer\", sep=\"\\t\"\n",
            ")\n",
            "df_time_series[\"datetime\"] = pd.to_datetime(df_time_series[\"datetime\"])\n",
            "# round the total number of subscribers, it is easier to consider 1 person and instead half of a person...\n",
            "df_time_series.subs = df_time_series.subs.round(0)\n",
            "\n",
            "# we filter the channels being labeled as \"Gaming\" in df_channels_en.tsv.gz\n",
            "df_time_series = df_time_series[df_time_series[\"channel\"].isin(channel_id_gaming)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bfb8de83-f104-4db7-a03d-45eedff3600d",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas = pd.read_feather(\n",
            "    \"./data/yt_metadata_helper.feather\",\n",
            "    columns=[\n",
            "        \"categories\",\n",
            "        \"upload_date\",\n",
            "        \"duration\",\n",
            "        \"like_count\",\n",
            "        \"dislike_count\",\n",
            "        \"view_count\",\n",
            "        \"channel_id\",\n",
            "        \"display_id\",\n",
            "    ],\n",
            ")\n",
            "\n",
            "# we added these two columns for analysis later on.\n",
            "video_metadatas[\"yearNumber\"] = video_metadatas[\"upload_date\"].dt.year\n",
            "video_metadatas[\"weekNumber\"] = video_metadatas[\"upload_date\"].dt.weekday\n",
            "\n",
            "# we filter the channels being labeled as \"Gaming\" in df_channels_en.tsv.gz\n",
            "video_metadatas = video_metadatas[video_metadatas[\"channel_id\"].isin(channel_id_gaming)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2d574dee",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.head()\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "3c27b9e8",
         "metadata": {},
         "source": [
            "We now have filtered ` ` df_channels `  ` , `  ` df_time_series `  ` and `  ` video_metadatas `  ` . However, they have different numbers of channels. `  ` df_channels `  ` and `  ` video_metadatas `  ` both have 20143 channels, whereas `  ` df_time_series ` ` has 19698 channels. \n",
            "\n",
            "When we look closer to the data, we can observe that `df_time_series` only has data from early January 2015 to end September 2019, which could be one reason causing the inconsistency. Therefore, we will then filter videos in ` ` video_metadatas ` ` that are being uploaded from early January 2015 to end September 2019 to keep consistency across three dataframes.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e3c22581",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas = video_metadatas[\n",
            "    (video_metadatas[\"upload_date\"] > df_time_series.datetime.min())\n",
            "    & (video_metadatas[\"upload_date\"] < df_time_series.datetime.max())\n",
            "]\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "81392b16",
         "metadata": {},
         "source": [
            "We now have a list of Gaming channels and the information about their videos. Now, we will proceed to filter out the channels that fit the following conditions:\n",
            "\n",
            "* The channel has more than XX % of gaming videos\n",
            "* The YouTuber uploaded with an average frequency of \"every XX days or less\" AND \"every XX days or more\"\n",
            "\n",
            "The reasoning behind these filtering criteria is that we want to have channels that are mainly focusing on gaming, and upload videos not too often but also not too infrequent. Channels with high frequency of upload rate is not likely to be achieved by a single person, and channels that upload too infrequent is likely to be abandoned.\n",
            "\n",
            "We will achieve this filtering by looking at ` ` video_metadatas ` ` , and calculate:\n",
            "\n",
            "* The number of videos being labelled as \"Gaming\" in each channel\n",
            "* The number of videos being uploaded to each channel\n",
            "* The time frame within which the YouTuber is active (corresponding to the number of days between the first and last uploads)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4c1e1626",
         "metadata": {},
         "outputs": [],
         "source": [
            "# The number of videos being labelled as \"Gaming\" in each channel\n",
            "video_metadatas[\"is_gaming\"] = video_metadatas.categories == \"Gaming\"\n",
            "\n",
            "# The number of videos being uploaded to each channel\n",
            "count_per_channel = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").count().categories)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"categories\": \"nr_videos\"})\n",
            ")\n",
            "game_video_count_per_channel = pd.DataFrame(\n",
            "    video_metadatas.groupby(\"channel_id\").sum().is_gaming\n",
            ").reset_index()\n",
            "\n",
            "# merge these two datasets\n",
            "channel_selection = count_per_channel.merge(game_video_count_per_channel)\n",
            "\n",
            "# The percentage of gaming videos within each channel\n",
            "channel_selection[\"pct_gaming\"] = (\n",
            "    channel_selection.is_gaming / channel_selection.nr_videos\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "609a1a0f",
         "metadata": {},
         "outputs": [],
         "source": [
            "pct_level = 0.6  # adjust the percentage of videos being labelled as \"Gaming\" here\n",
            "selected_channels = list(\n",
            "    channel_selection[channel_selection[\"pct_gaming\"] >= pct_level].channel_id\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b7c0b007",
         "metadata": {},
         "outputs": [],
         "source": [
            "len(selected_channels)\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "8760cb7c",
         "metadata": {},
         "source": [
            "After the filtering, we now have 18230 channels. \n",
            "\n",
            "We now want to filter out the channels that upload videos too frequently or too infrequent:\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "758656d4",
         "metadata": {},
         "outputs": [],
         "source": [
            "# The time frame within which the YouTuber is active\n",
            "last_vid_date = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").max().upload_date)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"upload_date\": \"last_vid_date\"})\n",
            ")\n",
            "first_vid_date = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").min().upload_date)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"upload_date\": \"first_vid_date\"})\n",
            ")\n",
            "total_vids = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").count().display_id)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"display_id\": \"total_vids\"})\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "003b816e",
         "metadata": {},
         "outputs": [],
         "source": [
            "channel_selection = first_vid_date.merge(last_vid_date)\n",
            "channel_selection = channel_selection.merge(total_vids)\n",
            "channel_selection[\"active_days\"] = (\n",
            "    channel_selection.last_vid_date - channel_selection.first_vid_date\n",
            ").dt.days\n",
            "channel_selection[\"upload_interval\"] = (\n",
            "    channel_selection.active_days / channel_selection.total_vids\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0fb8882b",
         "metadata": {},
         "outputs": [],
         "source": [
            "min_upload_interval = 2  # in days, the channel upload videos on average every x days\n",
            "max_upload_interval = 90\n",
            "selected_channels = list(\n",
            "    channel_selection[\n",
            "        (channel_selection[\"upload_interval\"] > min_upload_interval)\n",
            "        & (channel_selection[\"upload_interval\"] < max_upload_interval)\n",
            "    ].channel_id\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "87ef1e9c",
         "metadata": {},
         "outputs": [],
         "source": [
            "len(selected_channels)\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "c4e1889c",
         "metadata": {},
         "source": [
            "Here, we filtered out channels that uploaded videos every 2 days or less. Now we have 14730 channels.\n",
            "\n",
            "Note that `df_time_series` contains less channels than `df_channels` and `video_metadatas` . In order to keep consistency, we will take the channels that can be found across three dataframes.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1e138332",
         "metadata": {},
         "outputs": [],
         "source": [
            "df_time_series = df_time_series[df_time_series[\"channel\"].isin(selected_channels)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f085ddf6",
         "metadata": {},
         "outputs": [],
         "source": [
            "selected_channels = list(\n",
            "    df_time_series.groupby(\"channel\").count().reset_index().channel\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "23165ef0",
         "metadata": {},
         "outputs": [],
         "source": [
            "len(selected_channels)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d99d6ade",
         "metadata": {},
         "outputs": [],
         "source": [
            "df_channels = df_channels[df_channels[\"channel\"].isin(selected_channels)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "85fc232c",
         "metadata": {},
         "outputs": [],
         "source": [
            "len(df_channels)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8f80c1b3",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas = video_metadatas[video_metadatas[\"channel_id\"].isin(selected_channels)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ee68de9b",
         "metadata": {},
         "outputs": [],
         "source": [
            "len(video_metadatas.groupby(\"channel_id\").count())\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "c191d301",
         "metadata": {},
         "source": [
            "We also import a dataset that is created by us, .......  \n",
            "\n",
            "We will filter the channels and videos that we selected so far.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f6dc9dc4",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas = pd.read_feather(\"./data/yt_metadata_title_helper.feather\")\n",
            "\n",
            "title_metadatas = title_metadatas[\n",
            "    title_metadatas[\"display_id\"].isin(video_metadatas.display_id)\n",
            "]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e7c20891",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Save title_metadatas in feather\n",
            "title_metadatas.reset_index(inplace=True)\n",
            "title_metadatas.to_feather(\"./data/yt_metadata_title_filtered.feather\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8ce88e84",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "70ecc4d3",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "333d7f10",
         "metadata": {},
         "outputs": [],
         "source": [
            "tag_metadatas = pd.read_feather(\"./data/yt_metadata_tags_helper.feather\")\n",
            "\n",
            "tag_metadatas = tag_metadatas[\n",
            "    tag_metadatas[\"display_id\"].isin(video_metadatas.display_id)\n",
            "]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "99b37b20",
         "metadata": {},
         "outputs": [],
         "source": [
            "tag_metadatas.head()\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "38ee79b8",
         "metadata": {},
         "source": [
            "# Part 2: Further Analysis Milestone 3\n",
            "\n",
            "Given the results in Part 1 of our Analysis, we decided to focus on the categories of Gaming. The reason behind this choice is because we are trying to help our little brother to succeed, and since he is just a single person without a big team behind him it seems more reasonable than e.g. Movies or Film & Entertainment. Therefore, we decided to eliminate Music, How-to & Style, Education, Science & Technology, Entertainment, Film & Entertainment, People & Blogs, Comedy, Movies and Shows. We decided to investigate the three categories mentioned above also because videos uploaded can be addressed to audiences of all three categories.\n",
            "\n",
            "## Key Questions:\n",
            "\n",
            " - Which factors help a YouTuber in Gaming gain more subscribers?\n",
            " - How do sentiments in titles and tags affect views and how do these relationships change over time?\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "ac192b0a",
         "metadata": {},
         "source": [
            "### Sub-Question 1 \n",
            "\n",
            "**Description:** How does the video upload frequency, time of the week, and video length affect the subscription rate of the channels?  \n",
            "**Method:** Model this using a linear regression model.  \n",
            "**Timeline:** By 13/12/2022  \n",
            "**Organization:** Wenxiu  \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "6555e2c3",
         "metadata": {},
         "outputs": [],
         "source": [
            "# we also want to filter out channels that upload too frequently, as we are interested in things that could be done by an individual\n",
            "avg_upload_weekday = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").mean().weekNumber)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"weekNumber\": \"avg_upload_weekday\"})\n",
            ")\n",
            "\n",
            "avg_video_duration = (\n",
            "    pd.DataFrame(video_metadatas.groupby(\"channel_id\").mean().duration)\n",
            "    .reset_index()\n",
            "    .rename(columns={\"duration\": \"avg_video_duration\"})\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "be96957f",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0974b1c0",
         "metadata": {},
         "outputs": [],
         "source": [
            "channel_selection = channel_selection[\n",
            "    channel_selection[\"channel_id\"].isin(selected_channels)\n",
            "]\n",
            "channel_selection = channel_selection.merge(avg_upload_weekday)\n",
            "channel_selection = channel_selection.merge(avg_video_duration)\n",
            "channel_selection = channel_selection.merge(\n",
            "    df_channels.loc[:, [\"channel\", \"subscribers_cc\"]].rename(\n",
            "        columns={\"channel\": \"channel_id\"}\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2091ce77",
         "metadata": {},
         "outputs": [],
         "source": [
            "model_data = channel_selection.loc[\n",
            "    :, [\"upload_interval\", \"avg_upload_weekday\", \"avg_video_duration\", \"subscribers_cc\"]\n",
            "]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0845578a",
         "metadata": {},
         "outputs": [],
         "source": [
            "# normalise the data\n",
            "xmean = np.mean(model_data, axis=0)\n",
            "xstd = np.std(model_data, axis=0)\n",
            "Xbzs = (model_data - xmean) / xstd\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "aa6c9ecd",
         "metadata": {},
         "outputs": [],
         "source": [
            "Xbzs.head(9)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "970b5047",
         "metadata": {},
         "outputs": [],
         "source": [
            "import statsmodels.formula.api as smf\n",
            "\n",
            "# Declares the model\n",
            "mod = smf.ols(\n",
            "    formula=\"subscribers_cc ~ upload_interval + avg_upload_weekday + avg_video_duration\",\n",
            "    data=Xbzs,\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "cf8cedb7",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Fits the model (find the optimal coefficients, adding a random seed ensures consistency)\n",
            "np.random.seed(2)\n",
            "res = mod.fit()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9b13bd1e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Print thes summary output provided by the library.\n",
            "print(res.summary())\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "3e8e3805",
         "metadata": {},
         "source": [
            "upload interval and average video duration are statistically significant  \n",
            "* higher upload interval - lower subcription rate  \n",
            "* higher average video duration - lower subscription rate\n",
            "\n",
            "time of upload during the week doesn't matter that much\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "f180a346",
         "metadata": {},
         "source": [
            "### Sub-Question 2\n",
            "\n",
            "**Description:** How does the language used in titles affect subscription number?  \n",
            "**Method:** We will classify the sentiments of titles and tags using packages such as NLTK and try to see if this factor affects subscription number of the channels using relevant skills we learned in observational studies.  \n",
            "**Timeline:** By 15/12/2022  \n",
            "**Organization:** Wenxiu  \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "441cce77",
         "metadata": {},
         "outputs": [],
         "source": [
            "# NLP libraries\n",
            "import spacy, nltk, gensim, sklearn\n",
            "\n",
            "# Vader\n",
            "import vaderSentiment\n",
            "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
            "from gensim.models.phrases import Phrases\n",
            "from gensim.corpora import Dictionary\n",
            "from gensim.models import LdaMulticore\n",
            "import pyLDAvis.gensim_models\n",
            "\n",
            "# Initialise the Spacy analyzer in English\n",
            "nlp = spacy.load(\"en_core_web_sm\")\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "d47115f2",
         "metadata": {},
         "source": [
            "To start with, we will try to determine the common topics across all videos. This is done by performining topic detection using titles and tags of all selected videos.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "51fde3e2",
         "metadata": {},
         "outputs": [],
         "source": [
            "def topic_detection(dataset, datatype):\n",
            "    if datatype == \"title\":\n",
            "        new_list = []\n",
            "        # replace all underslash to space\n",
            "        for x in dataset:\n",
            "            new_text = x.replace(\"_\", \" \")\n",
            "            new_text = new_text.replace(\"-\", \" \")\n",
            "            new_text = new_text.replace(\"•\", \" \")\n",
            "            new_list.append(new_text.lower())\n",
            "\n",
            "    if datatype == \"tag\":\n",
            "        new_list = []\n",
            "        for x in dataset:\n",
            "            step1 = x.replace(\" \", \"_\")\n",
            "            step2 = step1.replace(\",\", \" \")\n",
            "            new_list.append(step2.lower())\n",
            "\n",
            "    STOPWORDS = spacy.lang.en.stop_words.STOP_WORDS\n",
            "\n",
            "    processed_docs = list()\n",
            "    for doc in nlp.pipe(new_list, n_process=5, batch_size=10):\n",
            "\n",
            "        # Process document using Spacy NLP pipeline.\n",
            "        ents = doc.ents  # Named entities\n",
            "\n",
            "        # Keep only words (no numbers, no punctuation).\n",
            "        # Lemmatize tokens, to lowercase, remove punctuation and remove stopwords.\n",
            "        doc = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
            "\n",
            "        # Remove common words from a stopword list and keep only words of length 3 or more.\n",
            "        doc = [token for token in doc if token not in STOPWORDS and len(token) > 2]\n",
            "\n",
            "        # Add named entities, but only if they are a compound of more than word.\n",
            "        doc.extend([str(entity) for entity in ents if len(entity) > 1])\n",
            "\n",
            "        processed_docs.append(doc)\n",
            "    docs = processed_docs\n",
            "    del processed_docs\n",
            "\n",
            "    # Add bigrams to docs (only ones that appear 15 times or more).\n",
            "    bigram = Phrases(docs, min_count=15)\n",
            "\n",
            "    for idx in range(len(docs)):\n",
            "        for token in bigram[docs[idx]]:\n",
            "            if \"_\" in token:\n",
            "                # Token is a bigram, add to document.\n",
            "                docs[idx].append(token)\n",
            "\n",
            "    # Create a dictionary representation of the documents, and filter out frequent and rare words.\n",
            "    dictionary = Dictionary(docs)\n",
            "\n",
            "    # Remove rare and common tokens.\n",
            "    # Filter out words that occur too frequently or too rarely.\n",
            "    max_freq = 0.5\n",
            "    min_wordcount = 5\n",
            "    dictionary.filter_extremes(no_below=min_wordcount, no_above=max_freq)\n",
            "\n",
            "    # Bag-of-words representation of the documents.\n",
            "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
            "\n",
            "    seed = 42\n",
            "    # models\n",
            "    params = {\"passes\": 10, \"random_state\": seed}\n",
            "    base_models = dict()\n",
            "    model = LdaMulticore(\n",
            "        corpus=corpus,\n",
            "        num_topics=4,\n",
            "        id2word=dictionary,\n",
            "        workers=6,\n",
            "        passes=params[\"passes\"],\n",
            "        random_state=params[\"random_state\"],\n",
            "    )\n",
            "    return model, corpus, dictionary\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "40a902d0",
         "metadata": {},
         "outputs": [],
         "source": [
            "# topic detection all videos from titles\n",
            "model, corpus, dictionary = topic_detection(title_metadatas.title, \"title\")\n",
            "data = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
            "pyLDAvis.display(data)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "77012ed0",
         "metadata": {},
         "outputs": [],
         "source": [
            "# topic detection for all videos from tags\n",
            "model, corpus, dictionary = topic_detection(tag_metadatas.tags, \"tag\")\n",
            "data = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
            "pyLDAvis.display(data)\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "77bb961f",
         "metadata": {},
         "source": [
            "We will then move on to determine the sentiments of titles, and their effect on the view counts.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2915df36",
         "metadata": {},
         "outputs": [],
         "source": [
            "positive_sent = []\n",
            "negative_sent = []\n",
            "neutral_sent = []\n",
            "compound_sent = []\n",
            "\n",
            "# iterate through the sentences, get polarity scores, choose a value\n",
            "analyzer = SentimentIntensityAnalyzer()\n",
            "for title in title_metadatas.title:\n",
            "    score = analyzer.polarity_scores(title)\n",
            "    positive_sent.append(score[\"pos\"])\n",
            "    negative_sent.append(score[\"neg\"])\n",
            "    neutral_sent.append(score[\"neu\"])\n",
            "    compound_sent.append(score[\"compound\"])\n",
            "\n",
            "video_metadatas[\"title_pos_scr\"] = positive_sent\n",
            "video_metadatas[\"title_neg_scr\"] = negative_sent\n",
            "video_metadatas[\"title_neu_scr\"] = neutral_sent\n",
            "video_metadatas[\"title_com_scr\"] = compound_sent\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8d60444d",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Write your code to make 4x4 panel here\n",
            "\n",
            "fig, ax = plt.subplots(5, 1, figsize=(12, 12), sharey=True, sharex=True)\n",
            "\n",
            "year = [2015, 2016, 2017, 2018, 2019]\n",
            "\n",
            "for i in np.arange(5):\n",
            "    sbplt = ax[i]\n",
            "    sbplt.hist(\n",
            "        video_metadatas[video_metadatas[\"yearNumber\"] == year[i]].title_com_scr, bins=15\n",
            "    )\n",
            "    sbplt.set_title(\"title compound score:\" + str(year[i]))\n",
            "\n",
            "\n",
            "fig.tight_layout()\n",
            "\n",
            "fig.text(0.4, 0, \"sentimental score\")\n",
            "fig.text(0, 0.6, \"Number of videos\", rotation=90)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3a42978f",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2ebf9915",
         "metadata": {},
         "outputs": [],
         "source": [
            "model_data = video_metadatas.loc[\n",
            "    :, [\"title_com_scr\", \"duration\", \"weekNumber\", \"view_count\"]\n",
            "]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "65660539",
         "metadata": {},
         "outputs": [],
         "source": [
            "# normalise the data\n",
            "xmean = np.mean(model_data, axis=0)\n",
            "xstd = np.std(model_data, axis=0)\n",
            "Xbzs = (model_data - xmean) / xstd\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fbb84400",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Declares the model\n",
            "mod = smf.ols(formula=\"view_count ~ title_com_scr + duration + weekNumber\", data=Xbzs)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2b0199a1",
         "metadata": {},
         "outputs": [],
         "source": [
            "# Fits the model (find the optimal coefficients, adding a random seed ensures consistency)\n",
            "np.random.seed(2)\n",
            "res = mod.fit()\n",
            "\n",
            "# Print thes summary output provided by the library.\n",
            "print(res.summary())\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "65e28dae",
         "metadata": {},
         "source": [
            "### Analyze the effect of the Capitalize words in title\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8e0c50ee",
         "metadata": {},
         "source": [
            "Lets implement the sub question 2, the goal is to find a relation btw the number of views and the titles (length, sentiment, casefold, etc...). For that an observational studies would be highly recommended to compare what is comparable. For that we will keep the channel id to see the number of subs of the channel and compare pairwise similar nb of subs. \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "cb21aab4",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas = pd.read_feather(\"./data/yt_metadata_title_filtered.feather\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e14e378a",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas.drop([\"index\", \"categories\"], axis=1, inplace=True)\n",
            "title_metadatas.head(2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9b871896",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas = pd.merge(video_metadatas, title_metadatas, on=\"display_id\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bfe861c1",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas.sample()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "67ed8e45",
         "metadata": {},
         "outputs": [],
         "source": [
            "# we need to improve the hw1 method a bit because we want to make sure that titles with a lot of spaces\n",
            "# don't have too many words and if there is a typo (e.g \"Welcome,in my video\" should be counted as 4 words and not 3)\n",
            "def count_words(x: str):\n",
            "    new_x = x.replace(\",\", \" \")\n",
            "    # change anything that isn’t an alphanumeric character or whitespace, and replaces it with a space\n",
            "    new_x = re.sub(r\"[^\\w\\s]\", \" \", new_x)\n",
            "    # Change many consecutive spaces into a single space\n",
            "    new_x = re.sub(\" +\", \" \", new_x)\n",
            "    # delete begin/end spaces\n",
            "    new_x = new_x.strip()\n",
            "    return len(new_x.split(\" \"))\n",
            "\n",
            "\n",
            "def get_freq_capital_words(sentence: str):\n",
            "    new_x = sentence.replace(\",\", \" \")\n",
            "    # change anything that isn’t an alphanumeric character or whitespace, and replaces it with a space\n",
            "    new_x = re.sub(r\"[^\\w\\s]\", \" \", new_x)\n",
            "    # Change many consecutive spaces into a single space\n",
            "    new_x = re.sub(\" +\", \" \", new_x)\n",
            "    # delete begin/end spaces\n",
            "    new_x = new_x.strip().split(\" \")\n",
            "    nb_capital_words = 0\n",
            "    for word in new_x:\n",
            "        if word.isupper():\n",
            "            nb_capital_words += 1\n",
            "    return nb_capital_words / len(new_x)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3ef63e39",
         "metadata": {},
         "outputs": [],
         "source": [
            "# test the function\n",
            "print(count_words(\"Salut     je suis la\"))\n",
            "print(count_words(\"Salut,je suis la\"))\n",
            "print(count_words(\"#FuckCancer | I'VE GOT SOME GREAT NEWS!\"))\n",
            "print(count_words(\"DISNEY CHRISTMAS VLOG! || Zak Longo\"))\n",
            "print(count_words(\"Sims 4 - SHOOTING SIMS WITH A GUN - The Sims 4\"))\n",
            "print(count_words(\"###ADA#ada##\"))\n",
            "\n",
            "# test capitalize words function\n",
            "print(get_freq_capital_words(\"Sims 4 - SHOOTING SIMS WITH A GUN - The Sims 4\"))\n",
            "print(get_freq_capital_words(\"###ADA###salut|||test&SALUT\"))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a5128965",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"title_nb_words\"] = title_metadatas.title.apply(\n",
            "    lambda title: count_words(title)\n",
            ")\n",
            "\n",
            "\n",
            "video_metadatas[\"freq_capitalize_words\"] = title_metadatas.title.apply(\n",
            "    lambda title: get_freq_capital_words(title)\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "bce87c54",
         "metadata": {},
         "outputs": [],
         "source": [
            "# distrib of the freq capitalize words.\n",
            "# process for the plot\n",
            "freq_capit_words = round(video_metadatas.freq_capitalize_words, 1)\n",
            "values_counts = freq_capit_words.value_counts().sort_index()\n",
            "values_counts.plot(kind=\"bar\")\n",
            "plt.title(\"distrib. of the freq. of capitalize words\")\n",
            "plt.xlabel(\"frequencies of upper word\")\n",
            "plt.ylabel(\"nb of titles\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "873301e1",
         "metadata": {},
         "outputs": [],
         "source": [
            "binary_freq_df = video_metadatas[\n",
            "    (video_metadatas.freq_capitalize_words == 0)\n",
            "    | (video_metadatas.freq_capitalize_words == 1)\n",
            "]\n",
            "\n",
            "no_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 0]\n",
            "full_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 1]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "889f721e",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"view_count_log\"] = np.log(\n",
            "    video_metadatas.view_count, where=video_metadatas.view_count != 0\n",
            ")\n",
            "mod = smf.ols(\n",
            "    formula=\"view_count_log ~ title_nb_words + freq_capitalize_words\",\n",
            "    data=video_metadatas,\n",
            ")\n",
            "res = mod.fit()\n",
            "print(res.summary())\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "565a324c",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas[\"title_nb_words\"] = metadatas.title.apply(lambda title: count_words(title))\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "46d08b7e",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas[\"freq_capitalize_words\"] = metadatas.title.apply(\n",
            "    lambda title: get_freq_capital_words(title)\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "66892f2f",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas.sample()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "8ffd323a",
         "metadata": {},
         "outputs": [],
         "source": [
            "# distrib of the freq capitalize words.\n",
            "# process for the plot\n",
            "freq_capit_words = round(metadatas.freq_capitalize_words, 1)\n",
            "values_counts = freq_capit_words.value_counts().sort_index()\n",
            "values_counts.plot(kind=\"bar\")\n",
            "plt.title(\"distrib. of the freq. of capitalize words\")\n",
            "plt.xlabel(\"frequencies of upper word\")\n",
            "plt.ylabel(\"nb of titles\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "62405130",
         "metadata": {},
         "outputs": [],
         "source": [
            "binary_freq_df = metadatas[\n",
            "    (metadatas.freq_capitalize_words == 0) | (metadatas.freq_capitalize_words == 1)\n",
            "]\n",
            "\n",
            "no_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 0]\n",
            "full_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 1]\n",
            "\n",
            "print(\"# of full upper words title\", full_upper_word_df.shape)\n",
            "print(\"# of full lower words title\", no_upper_word_df.shape)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "28750684",
         "metadata": {},
         "outputs": [],
         "source": [
            "# get the\n",
            "video_metadatas.head(2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d1c5c5d9",
         "metadata": {},
         "outputs": [],
         "source": [
            "metadatas[\"view_count_log\"] = np.log(\n",
            "    metadatas.view_count, where=metadatas.view_count != 0\n",
            ")\n",
            "mod = smf.ols(\n",
            "    formula=\"view_count_log ~ title_nb_words + freq_capitalize_words\", data=metadatas\n",
            ")\n",
            "res = mod.fit()\n",
            "print(res.summary())\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "7e589c32",
         "metadata": {},
         "source": [
            "The frequence of capitalize words in the titles is significant and it increases the number of views on the video. \\\n",
            "The number of words in the title is significant, however it doesnt seem to have a real impact on the number of views. \n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "7fe94c3c",
         "metadata": {},
         "source": [
            "#### Now match and compare the channels with ~ the same nb of subscribers\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c7ad3e5d",
         "metadata": {},
         "outputs": [],
         "source": [
            "sample_channels = df_channels[\n",
            "    (df_channels.subscribers_cc > 100_000) & (df_channels.subscribers_cc < 500_000)\n",
            "]\n",
            "sample_channels.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "07891a5d",
         "metadata": {},
         "outputs": [],
         "source": [
            "sample_metadatas = metadatas[metadatas.channel_id.isin(sample_channels.channel)]\n",
            "\n",
            "print(sample_metadatas.shape)\n",
            "len(sample_metadatas.groupby(\"channel_id\").count())\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "85118f31",
         "metadata": {},
         "outputs": [],
         "source": [
            "freq_capit_words = round(sample_metadatas.freq_capitalize_words, 1)\n",
            "values_counts = freq_capit_words.value_counts().sort_index()\n",
            "values_counts.plot(kind=\"bar\")\n",
            "plt.title(\"distrib. of the freq. of capitalize words\")\n",
            "plt.xlabel(\"frequencies of upper word\")\n",
            "plt.ylabel(\"nb of titles\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "62405130",
         "metadata": {},
         "outputs": [],
         "source": [
            "binary_freq_df = sample_metadatas[\n",
            "    (sample_metadatas.freq_capitalize_words == 0)\n",
            "    | (sample_metadatas.freq_capitalize_words == 1)\n",
            "]\n",
            "\n",
            "full_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 1]\n",
            "no_upper_word_df = binary_freq_df[binary_freq_df[\"freq_capitalize_words\"] == 0]\n",
            "\n",
            "print(\"# of full upper words title\", full_upper_word_df.shape)\n",
            "print(\"# of full lower words title\", no_upper_word_df.shape)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "121a914e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# select random rows of full lower words title to get same size of dataset\n",
            "no_upper_word_df = no_upper_word_df.sample(n=full_upper_word_df.shape[0])\n",
            "\n",
            "no_upper_word_df.shape\n",
            "\n",
            "frames = [full_upper_word_df, no_upper_word_df]\n",
            "\n",
            "dffff = pd.concat(frames)\n",
            "\n",
            "dffff.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d1c5c5d9",
         "metadata": {},
         "outputs": [],
         "source": [
            "mod = smf.ols(\n",
            "    formula=\"view_count_log ~ title_nb_words + freq_capitalize_words\", data=dffff\n",
            ")\n",
            "res = mod.fit()\n",
            "print(res.summary())\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "2f25907e",
         "metadata": {},
         "source": [
            "Lets plot the upper and lowers title for big channels (> 100 000 subs) and others. \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "05167070",
         "metadata": {},
         "outputs": [],
         "source": [
            "big_channels = df_channels[df_channels.subscribers_cc > 100_000]\n",
            "little_channels = df_channels[df_channels.subscribers_cc <= 100_000]\n",
            "\n",
            "print(big_channels.shape)\n",
            "print(little_channels.shape)\n",
            "print(len(metadatas.groupby(\"channel_id\").count()))\n",
            "big_channels.sample()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "edb2b82f",
         "metadata": {},
         "outputs": [],
         "source": [
            "big_videos_meta = metadatas[metadatas.channel_id.isin(big_channels.channel)]\n",
            "little_videos_meta = metadatas[metadatas.channel_id.isin(little_channels.channel)]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "f38ac39b",
         "metadata": {},
         "outputs": [],
         "source": [
            "print(big_videos_meta.freq_capitalize_words.mean())\n",
            "print(little_videos_meta.freq_capitalize_words.mean())\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "5894abc1",
         "metadata": {},
         "source": [
            "### Sub-Question 3\n",
            "\n",
            "**Description:** Can we predict the channel's success based on channel information, including average video length, upload frequency, usual time of uploads, categories of videos uploaded, positive/negative sentiments of the title, person pronouns to address the viewers, the number of words in the title, and the number of tags used?  \n",
            "**Method:** We can implement kNN method or Random Forests to train the dataset.  \n",
            "**Timeline:** By 13/12/2022  \n",
            "**Organization:** Dorothee  \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "939ae24c",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"duration_min\"] = video_metadatas[\"duration\"] / 60\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "c3581fdb",
         "metadata": {},
         "source": [
            "Add a column to know the day of the week of the upload date for the video\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "2fdd660c",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"week_number_mean\"] = video_metadatas.groupby(\"channel_id\")[\n",
            "    \"weekNumber\"\n",
            "].transform(\"mean\")\n",
            "\n",
            "video_metadatas[\"week_number_mean\"] = video_metadatas[\"week_number_mean\"].astype(\"int\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e92e25a9",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"mean_duration_min\"] = video_metadatas.groupby(\"channel_id\")[\n",
            "    \"duration_min\"\n",
            "].transform(\"mean\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fab16719",
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_nb_unique_cat(x):\n",
            "    return x.nunique()\n",
            "\n",
            "\n",
            "def get_cat(x):\n",
            "    return list(x.unique())\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "71821d3d",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"nb_diff_cat\"] = video_metadatas.groupby(\"channel_id\")[\n",
            "    \"categories\"\n",
            "].transform(get_nb_unique_cat)\n",
            "\n",
            "cat_per_channels = (\n",
            "    video_metadatas.groupby(\"channel_id\")[\"categories\"].unique().to_frame()\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9e40512f",
         "metadata": {},
         "outputs": [],
         "source": [
            "cat_per_channels = cat_per_channels.reset_index()\n",
            "cat_per_channels = cat_per_channels.rename({\"categories\": \"covered_categories\"}, axis=1)\n",
            "cat_per_channels.head(2)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "10f92779",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas = pd.merge(video_metadatas, cat_per_channels, on=\"channel_id\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9fc00227",
         "metadata": {},
         "outputs": [],
         "source": [
            "# group by channels and week of upload date to calculate the mean upload frequencies of the channels\n",
            "grouped = video_metadatas.groupby(\n",
            "    [\"channel_id\", pd.Grouper(key=\"upload_date\", freq=\"W\")]\n",
            ").size()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "7fb5e3ea",
         "metadata": {},
         "outputs": [],
         "source": [
            "week_frequency_mean = (\n",
            "    grouped.groupby(level=\"channel_id\").mean().to_frame().reset_index()\n",
            ")\n",
            "week_frequency_mean = week_frequency_mean.rename(columns={0: \"week_frequency_mean\"})\n",
            "video_metadatas = pd.merge(video_metadatas, week_frequency_mean, on=\"channel_id\")\n",
            "week_frequency_mean\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b767603e",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"title_com_scr_mean\"] = video_metadatas.groupby(\"channel_id\")[\n",
            "    \"title_com_scr\"\n",
            "].transform(\"mean\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "27eb3718",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas[\"title_nb_words_mean\"] = video_metadatas.groupby(\"channel_id\")[\n",
            "    \"title_nb_words\"\n",
            "].transform(\"mean\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e648bbd9",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas = pd.read_feather(\"./data/videos_metadatas_sub_quest_3_v2\")\n",
            "\n",
            "print(video_metadatas.shape)\n",
            "video_metadatas.head()\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "da4c5794",
         "metadata": {},
         "source": [
            "Now find the pronouns in the title\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "b2cb7e7a",
         "metadata": {},
         "outputs": [],
         "source": [
            "feature_wordsets = dict(\n",
            "    [\n",
            "        # https://en.wikipedia.org/wiki/English_personal_pronouns\n",
            "        (\n",
            "            \"first_person_singular\",\n",
            "            [\n",
            "                \"i\",\n",
            "                \"me\",\n",
            "                \"my\",\n",
            "                \"mine\",\n",
            "                \"myself\",\n",
            "                \"i'd\",\n",
            "                \"i'll\",\n",
            "                \"i'm\",\n",
            "                \"i've\",\n",
            "                \"id\",\n",
            "                \"im\",\n",
            "                \"ive\",\n",
            "            ],\n",
            "        ),\n",
            "        (\n",
            "            \"first_person_plural\",\n",
            "            [\n",
            "                \"we\",\n",
            "                \"us\",\n",
            "                \"our\",\n",
            "                \"ours\",\n",
            "                \"ourselves\",\n",
            "                \"we'd\",\n",
            "                \"we'll\",\n",
            "                \"we're\",\n",
            "                \"we've\",\n",
            "            ],\n",
            "        ),\n",
            "        (\n",
            "            \"second_person\",\n",
            "            [\n",
            "                \"you\",\n",
            "                \"your\",\n",
            "                \"yours\",\n",
            "                \"yourself\",\n",
            "                \"ya\",\n",
            "                \"you'd\",\n",
            "                \"you'll\",\n",
            "                \"you're\",\n",
            "                \"you've\",\n",
            "                \"youll\",\n",
            "                \"youre\",\n",
            "                \"youve\",\n",
            "                \"yourselves\",\n",
            "            ],\n",
            "        ),\n",
            "        (\n",
            "            \"third_person_singular\",\n",
            "            [\n",
            "                \"he\",\n",
            "                \"him\",\n",
            "                \"his\",\n",
            "                \"himself\",\n",
            "                \"he'd\",\n",
            "                \"he's\",\n",
            "                \"hes\",\n",
            "                \"she\",\n",
            "                \"her\",\n",
            "                \"hers\",\n",
            "                \"herself\",\n",
            "                \"she'll\",\n",
            "                \"she's\",\n",
            "                \"shes\",\n",
            "                \"it\",\n",
            "                \"its\",\n",
            "                \"itself\",\n",
            "                \"themself\",\n",
            "            ],\n",
            "        ),\n",
            "        (\n",
            "            \"third_person_plural\",\n",
            "            [\n",
            "                \"they\",\n",
            "                \"them\",\n",
            "                \"their\",\n",
            "                \"theirs\",\n",
            "                \"themselves\",\n",
            "                \"they'd\",\n",
            "                \"they'll\",\n",
            "                \"they've\",\n",
            "                \"theyll\",\n",
            "                \"theyve\",\n",
            "            ],\n",
            "        ),\n",
            "    ]\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "82cdf39b",
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_new_columns_pronouns(feature_wordsets, df, colname):\n",
            "    dict_pronoun_list = dict(\n",
            "        [\n",
            "            (\"first_person_singular\", []),\n",
            "            (\"first_person_plural\", []),\n",
            "            (\"second_person\", []),\n",
            "            (\"third_person_singular\", []),\n",
            "            (\"third_person_plural\", []),\n",
            "        ]\n",
            "    )\n",
            "    for headline in df[colname]:\n",
            "        headline = headline.lower()\n",
            "        for key in feature_wordsets.keys():\n",
            "            # if a word corresponding to a pronoun is present in the headline,\n",
            "            # then add 1 to this pronoun list (column) otherwise add 0\n",
            "            if any(\n",
            "                re.search(r\"\\b\" + pronoun + r\"\\b\", headline)\n",
            "                for pronoun in feature_wordsets[key]\n",
            "            ):\n",
            "                dict_pronoun_list[key].append(1)\n",
            "            else:\n",
            "                dict_pronoun_list[key].append(0)\n",
            "    return dict_pronoun_list\n",
            "\n",
            "\n",
            "dict_pronoun_list = get_new_columns_pronouns(feature_wordsets, title_metadatas, \"title\")\n",
            "\n",
            "# create the new columns in our dataframe\n",
            "for col in dict_pronoun_list.keys():\n",
            "    title_metadatas[col] = dict_pronoun_list[col]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ab71c514",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas.head(3)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "1eca3e8c",
         "metadata": {},
         "outputs": [],
         "source": [
            "title_metadatas_without_titles = title_metadatas.drop(\"title\", axis=1)\n",
            "title_metadatas_without_titles\n",
            "\n",
            "video_metadatas = pd.merge(\n",
            "    video_metadatas, title_metadatas_without_titles, on=\"display_id\"\n",
            ")\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "5d89b4e0",
         "metadata": {},
         "source": [
            "### Sub-Question 4\n",
            "\n",
            "**Description:** What are the most common topics in each of the chosen categories?  \n",
            "**Method:** The yt_metadata_en.jsonl.gz dataset will be used to get a list of tags of each video according to its category. It is further split and classified according to the topics that occur most frequently. This way, we get the most used keywords in each video category and therefore the most popular topics.  \n",
            "**Timeline:** By 15/12/2022  \n",
            "**Organization:** Jules  \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "272014e0",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "4640ed6f",
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "2600e95a",
         "metadata": {},
         "source": [
            "### Sub-Question 5\n",
            "\n",
            "**Description:** Does a channel's success increase with a greater variety of categories?  \n",
            "**Method:** For this question, we will determine whether the filtered channels use multiple categories in their videos, and if they showed clear shifts from one category to another. Ultimately, we want to use this information and methods such as A/B testing and observational studies to determine whether a greater variety of categories can aid to a channels’ success.  \n",
            "**Timeline:** By 18/12/2022  \n",
            "**Organization:** Paul  \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0e534ce3",
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.categories.value_counts().plot(kind=\"bar\")\n",
            "plt.title(\"Number of videos per categories for channels with Gaming topic\")\n",
            "plt.xlabel(\"Categories\")\n",
            "plt.ylabel(\"Number of videos\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "63be44ea",
         "metadata": {},
         "source": [
            "Find channels with unique topic\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "530d6927",
         "metadata": {},
         "outputs": [],
         "source": [
            "gaming_channels_with_categories = video_metadatas.groupby(\"channel_id\").apply(\n",
            "    lambda group: group.categories.unique()\n",
            ")\n",
            "unique_topic_gaming = gaming_channels_with_categories.loc[\n",
            "    gaming_channels_with_categories.str.len() == 1\n",
            "]\n",
            "non_unique_topic_gaming = gaming_channels_with_categories.loc[\n",
            "    gaming_channels_with_categories.str.len() > 1\n",
            "]\n",
            "print(\"nb of channels with unique topic Gaming : \", unique_topic_gaming.shape)\n",
            "print(\"nb of channels without unique topic Gaming : \", non_unique_topic_gaming.shape)\n",
            "unique_topic_gaming.value_counts()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "153bc452",
         "metadata": {},
         "outputs": [],
         "source": [
            "unique_topic_gaming_channels_descript = df_channels[\n",
            "    df_channels.channel.isin(unique_topic_gaming.index.values)\n",
            "]\n",
            "non_unique_topic_gaming_channels_descript = df_channels[\n",
            "    df_channels.channel.isin(non_unique_topic_gaming.index.values)\n",
            "]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "93b03f1f",
         "metadata": {},
         "source": [
            "### Now we have the dataset to check the differences between unique and non unique categories channels\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "52a7f632",
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_confidence_interval(data):\n",
            "    rng = np.random.default_rng()\n",
            "    return bootstrap(\n",
            "        (data,),\n",
            "        np.mean,\n",
            "        confidence_level=0.95,\n",
            "        random_state=rng,\n",
            "    ).confidence_interval\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0a1c47d2",
         "metadata": {},
         "outputs": [],
         "source": [
            "conf1 = get_confidence_interval(unique_topic_gaming_channels_descript.subscribers_cc)\n",
            "print(\n",
            "    \"95% confidence interval of number of subs for unique Gaming channels [{:_}, {:_}]\".format(\n",
            "        int(conf1[0]), int(conf1[1])\n",
            "    )\n",
            ")\n",
            "conf2 = get_confidence_interval(\n",
            "    non_unique_topic_gaming_channels_descript.subscribers_cc\n",
            ")\n",
            "print(\n",
            "    \"95% confidence interval of number of subs for non unique Gaming channels [{:_}, {:_}]\".format(\n",
            "        int(conf2[0]), int(conf2[1])\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "id": "15286dd9",
         "metadata": {},
         "source": [
            "From these statistics, we can notice that diverse channels have no more subs in general. \n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.13"
      },
      "vscode": {
         "interpreter": {
            "hash": "e2552cbddfdf66195580a0cc7030164c04fc13f4bfb78773728a78bbf4f04323"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
