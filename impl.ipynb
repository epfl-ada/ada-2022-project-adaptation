{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.font_manager as font_manager\n",
            "from matplotlib.lines import Line2D\n",
            "import matplotlib as mpl\n",
            "import matplotlib.ticker as mtick\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import seaborn as sns\n",
            "import os\n",
            "from scipy.stats import bootstrap\n",
            "import statsmodels.api as sm\n",
            "import statsmodels.formula.api as smf\n",
            "\n",
            "params = {\n",
            "    \"axes.titlesize\": 14,\n",
            "    \"axes.labelsize\": 12,\n",
            "    \"font.size\": 12,\n",
            "    \"legend.fontsize\": 12,\n",
            "    \"xtick.labelsize\": 12,\n",
            "    \"ytick.labelsize\": 12,\n",
            "    \"text.usetex\": False,\n",
            "}\n",
            "\n",
            "mpl.rcParams.update(params)\n",
            "\n",
            "\n",
            "import warnings\n",
            "\n",
            "warnings.filterwarnings(\"ignore\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_time_series = pd.read_csv(\n",
            "    \"./data/df_timeseries_en.tsv.gz\", compression=\"infer\", sep=\"\\t\"\n",
            ")\n",
            "df_time_series[\"datetime\"] = pd.to_datetime(df_time_series[\"datetime\"])\n",
            "\n",
            "df_channels = pd.read_csv(\"./data/df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
            "df_channels[\"join_date\"] = pd.to_datetime(df_channels[\"join_date\"])\n",
            "video_metadatas = pd.read_feather(\n",
            "    \"./data/yt_metadata_helper.feather\",\n",
            "    columns=[\n",
            "        \"categories\",\n",
            "        \"upload_date\",\n",
            "        \"duration\",\n",
            "        \"like_count\",\n",
            "        \"view_count\",\n",
            "        \"channel_id\",\n",
            "    ],\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# round the subs value, easier to consider 1 sub as a person and not half of a person...\n",
            "df_time_series.subs = df_time_series.subs.round(0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_time_series.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df_channels.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.head()\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Check length of videos"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "An idea would be to use cut to match length of videos into discrete intervals "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.duration.describe()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# To make it easier to grasp the length of videos, we will convert the lengths to minutes at first...\n",
            "video_metadatas[\"duration_min\"] = video_metadatas[\"duration\"] / 60\n",
            "video_metadatas.duration_min.describe()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.boxplot(video_metadatas[\"duration_min\"], showfliers=False, vert=False)\n",
            "plt.title(\"Duration of Videos in Minutes\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "duration_min_cumul = plt.hist(\n",
            "    video_metadatas.duration_min, bins=100, log=True, cumulative=-1, histtype=\"step\"\n",
            ")\n",
            "plt.title(\"Histogram of Duration for YouTube Videos (cumulative)\")\n",
            "plt.ylabel(\"# of Videos (in log scale)\")\n",
            "plt.xlabel(\"Duration in Minutes\")\n",
            "plt.xticks(rotation=30, ha=\"right\")\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.loglog(duration_min_cumul[1][1:], duration_min_cumul[0])\n",
            "plt.title(\"Histogram of Duration for YouTube Videos (cumulative)\")\n",
            "plt.ylabel(\"# of Videos (in log scale)\")\n",
            "plt.xlabel(\"Duration\")\n",
            "plt.xticks(rotation=30, ha=\"right\")\n",
            "plt.show()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# long to compute\n",
            "\n",
            "# bucket_durations = pd.cut(video_metadatas['duration'], bins=100)\n",
            "# print(type(bucket_durations))\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Check frequency of videos"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_count = (\n",
            "    video_metadatas.groupby(pd.Grouper(key=\"upload_date\", freq=\"W\")).count().channel_id\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plt.plot(video_count, color=\"#7570b3\", ls=\"--\", label=\"\\% videos uploaded\")\n",
            "plt.yscale(\"log\")\n",
            "plt.ylabel(\"Number of Videos\")\n",
            "plt.xlabel(\"Time\")\n",
            "plt.title(\"Number of videos updated each week\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# now we will determine how many videos are uploaded to YouTube by each channel by year\n",
            "video_metadatas[\"yearkNumber\"] = video_metadatas[\"upload_date\"].dt.year\n",
            "vd_cnt_by_id_mo = (\n",
            "    video_metadatas.groupby([\"channel_id\", \"yearkNumber\"])\n",
            "    .count()\n",
            "    .categories.unstack(level=1)\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vd_cnt_by_id_mo.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "sns.heatmap(vd_cnt_by_id_mo, yticklabels=False)\n",
            "# think about better representation of the data and possibly get rid of channels with extremely low uploads--- to do\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# How long to reach 1M subscribers "
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Could be interesting to take all the videos with 10K, 20K, 30K, ... and see the evolution to reach 100K for example. And compare different evolution : from 50K to 100K or from 500K to 600K, etc.. which one is the fastest, easiest ?  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_df_channel_ascension(start_subs, end_subs):\n",
            "    \"\"\"finds the channels in the time series that have less than the start_subs and more than end_subs.\n",
            "    It means that it begins with less than start_subs and have now at least end_subs\n",
            "\n",
            "    Args:\n",
            "        start_subs (float): low_threshold\n",
            "        end_subs (float): high_threshold\n",
            "\n",
            "    Returns:\n",
            "        dataframe containing all the channels with evolution from starts_subs to end_subs\n",
            "    \"\"\"\n",
            "    under_start_subs = df_time_series[df_time_series[\"subs\"] < start_subs]\n",
            "    more_end_subs = df_time_series[df_time_series[\"subs\"] > end_subs]\n",
            "    channel_start_to_end = df_time_series[\n",
            "        df_time_series.channel.isin(under_start_subs.channel)\n",
            "        & df_time_series.channel.isin(more_end_subs.channel)\n",
            "    ]\n",
            "    return channel_start_to_end\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def get_mean_time_to_reach_X_subs(df, lower_bound, upper_bound):\n",
            "    time_to_reach_X_subs = df.groupby(\"channel\").apply(\n",
            "        lambda group: pd.Series(\n",
            "            {\n",
            "                \"from_zero_to_hero_duration\": group[group.subs > upper_bound][\n",
            "                    \"datetime\"\n",
            "                ].iloc[0]\n",
            "                - group[group.subs < lower_bound][\"datetime\"].iloc[-1]\n",
            "            }\n",
            "        )\n",
            "    )\n",
            "    return time_to_reach_X_subs\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "channel_10K_to_1M = get_df_channel_ascension(10_000, 1_000_000)\n",
            "print(\n",
            "    \"We have {} channels that begins with 10K subs and reach at least 1M\".format(\n",
            "        channel_10K_to_1M.channel.nunique()\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "first_channel = channel_10K_to_1M.iloc[0]\n",
            "test = channel_10K_to_1M[channel_10K_to_1M[\"channel\"] == first_channel.channel]\n",
            "# test = test[(test['datetime'] > np.datetime64('2018-07-20')) & (test['datetime'] < np.datetime64('2019-01-20'))]\n",
            "test.plot(x=\"datetime\", y=\"subs\")\n",
            "print(\"There are {} weeks for this channel\".format(test.shape[0]))\n",
            "print(\n",
            "    \"We should have approximately the same value : \\n{} and {}\".format(\n",
            "        7 * test.shape[0], test.iloc[-1].datetime - test.iloc[0].datetime\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# get the time it takes to go from less than 10K to 1M\n",
            "time_to_reach_1M = get_mean_time_to_reach_X_subs(channel_10K_to_1M, 10_000, 1_000_000)\n",
            "time_to_reach_1M.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(\n",
            "    \"In average YTbers take {} days to reach 1M of subscribers\".format(\n",
            "        time_to_reach_1M[\"from_zero_to_hero_duration\"].mean().days\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "It would be nice to bootstrap this result to see with interval of confidence the time taken by Youtuber to reach 1M"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data = time_to_reach_1M[\"from_zero_to_hero_duration\"].apply(\n",
            "    lambda delta_time: delta_time.days\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "rng = np.random.default_rng()\n",
            "conf_interval_reach_1M = bootstrap(\n",
            "    (data,),\n",
            "    np.mean,\n",
            "    confidence_level=0.95,\n",
            "    random_state=rng,\n",
            ").confidence_interval\n",
            "conf_interval_reach_1M\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(\n",
            "    \"The 95% interval of confidence, the time taken to reach 1M (from 10K) is [{}, {}]\".format(\n",
            "        conf_interval_reach_1M.low, conf_interval_reach_1M.high\n",
            "    )\n",
            ")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Check the number of views (maybe after we could add likes, dislike) given a certain categories and date of videos"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "For this task lets analyze and after ploting and getting some intuition, use logistic regression to 'predict' the nb of views given category and date "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas.head()\n",
            "video_metadatas.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# delete the rows with empty categories\n",
            "videos_with_cat = video_metadatas\n",
            "videos_with_cat.categories.replace(\"\", np.nan, inplace=True)\n",
            "videos_with_cat.dropna(subset=[\"categories\"], inplace=True)\n",
            "videos_with_cat.shape\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "encoded_cat = pd.get_dummies(videos_with_cat.categories)\n",
            "video_metadatas_encoded = videos_with_cat.join(encoded_cat)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas_encoded[\"year\"] = video_metadatas_encoded[\"upload_date\"].dt.year\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "mean_views = video_metadatas_encoded.groupby([\"year\", \"categories\"]).apply(\n",
            "    lambda x: pd.Series({\"mean_view\": x.view_count.mean()})\n",
            ")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "mean_views[:10]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# see the top most viewed categories every year\n",
            "largest_cat_every_year = (\n",
            "    mean_views.groupby(\"year\")[\"mean_view\"].nlargest(2).droplevel(0)\n",
            ")\n",
            "largest_cat_every_year[:10]\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Use logsitic regression to predict the number of views given the category and the date"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas_encoded.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "video_metadatas_encoded.columns\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = smf.ols(\n",
            "    formula='view_count ~ year + C(Autos & Vehicles) + C(Comedy) + C(Education) + C(Entertainment) + C(Film & Animation) + \\\n",
            "                C(Gaming) + C(Movies) + C(Music) + C(News & Politics) + C(Nonprofits & Activism) + \\\n",
            "                C(People & Blogs) + C(Pets & Animals) + C(Science & Technology) + C(Shows) + C(Sports) + C(Travel & Events)',\n",
            "    data=video_metadatas_encoded\n",
            ")\n",
            "\n",
            "res = model.fit()\n",
            "print(res.summary())\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "---"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "top_ranked_channels = df_channels[df_channels.subscriber_rank_sb < 100]\n",
            "top_ranked_channels.head()\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "date = top_ranked_channels[\"join_date\"].apply(lambda d: d.to_pydatetime().year)\n",
            "np.mean(date, axis=0)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "oldest_video = df_channels[\"join_date\"].apply(lambda d: d.to_pydatetime().year)\n",
            "oldest_video.nsmallest(5)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "channels_with_largest_subscribers = df_channels.nlargest(53, \"subscribers_cc\")\n",
            "channels_with_largest_subscribers.sample(7)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.13"
      },
      "vscode": {
         "interpreter": {
            "hash": "e2552cbddfdf66195580a0cc7030164c04fc13f4bfb78773728a78bbf4f04323"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
