{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib as mpl\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import bootstrap\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "params = {\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"font.size\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"text.usetex\": False,\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_time_series \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/df_timeseries_en.tsv.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m df_time_series[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df_time_series[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      6\u001b[0m df_channels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/df_channels_en.tsv.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:847\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1960\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "df_time_series = pd.read_csv(\n",
    "    \"./data/df_timeseries_en.tsv.gz\", compression=\"infer\", sep=\"\\t\"\n",
    ")\n",
    "df_time_series[\"datetime\"] = pd.to_datetime(df_time_series[\"datetime\"])\n",
    "\n",
    "df_channels = pd.read_csv(\"./data/df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
    "df_channels[\"join_date\"] = pd.to_datetime(df_channels[\"join_date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas = pd.read_feather(\n",
    "    \"./data/yt_metadata_helper.feather\",\n",
    "    columns=[\n",
    "        \"categories\",\n",
    "        \"upload_date\",\n",
    "        \"duration\",\n",
    "        \"like_count\",\n",
    "        \"dislike_count\",\n",
    "        \"view_count\",\n",
    "        \"channel_id\",\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round the total number of subscribers, it is easier to consider 1 person and instead half of a person...\n",
    "df_time_series.subs = df_time_series.subs.round(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check length of videos uploaded on YouTube between 2005-05-24 to 2019-11-20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idea would be to use **cut** to match length of videos into discrete intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas.duration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it easier, we convert the lengths of videos,to minutes...\n",
    "video_metadatas[\"duration_min\"] = video_metadatas[\"duration\"] / 60\n",
    "video_metadatas.duration_min.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(video_metadatas[\"duration_min\"], showfliers=False, vert=False)\n",
    "plt.title(\"Duration of Videos in Minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_min_cumul = plt.hist(\n",
    "    video_metadatas.duration_min, bins=100, log=True, cumulative=-1, histtype=\"step\"\n",
    ")\n",
    "plt.title(\"Histogram of Duration for YouTube Videos (cumulative)\")\n",
    "plt.ylabel(\"# of Videos (in log scale)\")\n",
    "plt.xlabel(\"Duration in Minutes\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(duration_min_cumul[1][1:], duration_min_cumul[0])\n",
    "plt.title(\"Histogram of Duration for YouTube Videos (cumulative)\")\n",
    "plt.ylabel(\"# of Videos (in log scale)\")\n",
    "plt.xlabel(\"Duration\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long to compute\n",
    "\n",
    "# bucket_durations = pd.cut(video_metadatas['duration'], bins=100)\n",
    "# print(type(bucket_durations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check frequency of videos uploaded on YouTube between 2005-05-24 to 2019-11-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_count = (\n",
    "    video_metadatas.groupby(pd.Grouper(key=\"upload_date\", freq=\"W\")).count().channel_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(video_count, color=\"#7570b3\", ls=\"--\", label=\"\\% videos uploaded\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylabel(\"Number of Videos\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.title(\"Number of videos uploaded each week\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following graph shows how many videos each channel uploads to YouTube per year.\n",
    "video_metadatas[\"yearNumber\"] = video_metadatas[\"upload_date\"].dt.year\n",
    "vd_cnt_by_id_yr = (\n",
    "    video_metadatas.groupby([\"channel_id\", \"yearNumber\"])\n",
    "    .count()\n",
    "    .categories.unstack()\n",
    "    .reset_index()\n",
    ")\n",
    "vd_cnt_by_id_yr.columns.name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the channels that uploaded less than 12 videos throughout the study period. JUSTIFY WHY 12 OR DELAY / DELETE\n",
    "\n",
    "vd_cnt_by_id_yr_filtered = vd_cnt_by_id_yr[vd_cnt_by_id_yr.sum(axis=1) > 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_cnt_by_id_yr_filtered_summary = vd_cnt_by_id_yr_filtered.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(\n",
    "    x=list(vd_cnt_by_id_yr_filtered_summary.columns.values),\n",
    "    y1=list(vd_cnt_by_id_yr_filtered_summary.loc[\"25%\", :]),\n",
    "    y2=list(vd_cnt_by_id_yr_filtered_summary.loc[\"75%\", :]),\n",
    "    alpha=0.5,\n",
    "    color=\"gray\",\n",
    ")\n",
    "plt.plot(vd_cnt_by_id_yr_filtered_summary.loc[\"50%\", :], color=\"black\")\n",
    "\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Yearly Upload Frequency\")\n",
    "plt.title(\"The 2nd and 3rd quartiles of yearly video upload frequency\")\n",
    "# here we choose quartiles, since the mean would be significantly affected by extremely high yearly upload rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekday Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas[\"weekNumber\"] = video_metadatas[\"upload_date\"].dt.weekday\n",
    "vd_cnt_by_id_wk = (\n",
    "    video_metadatas.groupby([\"channel_id\", \"weekNumber\", \"yearNumber\"])\n",
    "    .count()\n",
    "    .categories.unstack()\n",
    "    .reset_index()\n",
    ")\n",
    "vd_cnt_by_id_wk.columns.name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_cnt_by_id_wk[\"mean_upload\"] = vd_cnt_by_id_wk.iloc[:, 2:].mean(axis=1, skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\", \"SUN\"]\n",
    "ax = sns.boxplot(\n",
    "    x=\"weekNumber\", y=\"mean_upload\", data=vd_cnt_by_id_wk, showfliers=False\n",
    ").set(\n",
    "    xlabel='Week Days', \n",
    "    ylabel='Mean Upload',\n",
    "    xticklabels= labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_cnt_by_wk = (\n",
    "    video_metadatas.groupby([\"weekNumber\", \"yearNumber\"])\n",
    "    .count()\n",
    "    .categories.unstack()\n",
    "    .reset_index()\n",
    ")\n",
    "vd_cnt_by_wk.columns.name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd_cnt_by_wk_norm = vd_cnt_by_wk.iloc[:, 2:] / vd_cnt_by_wk.iloc[:, 2:].sum(skipna=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "plt.plot(vd_cnt_by_wk_norm.T)\n",
    "plt.legend(\n",
    "    [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    ")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"NORMALISED: video uploaded each weekday\")\n",
    "plt.title(\"NORMALISED: The evolution of video upload rate for each weekday\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that weekdays are prefered day to upload videos compared to weekends. However, which day of the week doesn't seem to matter too much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How long does it take to reach 1M subscribers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could be interesting to take all the videos with 10K, 20K, 30K, ... and see the evolution to reach 100K for example. And compare different evolution : from 50K to 100K or from 500K to 600K, etc.. which one is the fastest, easiest ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_channel_ascension(start_subs, end_subs):\n",
    "    \"\"\"finds the channels in the time series that have less than the start_subs and more than end_subs.\n",
    "    It means that it begins with less than start_subs and have now at least end_subs\n",
    "\n",
    "    Args:\n",
    "        start_subs (float): low_threshold\n",
    "        end_subs (float): high_threshold\n",
    "\n",
    "    Returns:\n",
    "        dataframe containing all the channels with evolution from starts_subs to end_subs\n",
    "    \"\"\"\n",
    "    under_start_subs = df_time_series[df_time_series[\"subs\"] < start_subs]\n",
    "    more_end_subs = df_time_series[df_time_series[\"subs\"] > end_subs]\n",
    "    channel_start_to_end = df_time_series[\n",
    "        df_time_series.channel.isin(under_start_subs.channel)\n",
    "        & df_time_series.channel.isin(more_end_subs.channel)\n",
    "    ]\n",
    "    return channel_start_to_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_time_to_reach_X_subs(df, lower_bound, upper_bound):\n",
    "    time_to_reach_X_subs = df.groupby(\"channel\").apply(\n",
    "        lambda group: pd.Series(\n",
    "            {\n",
    "                \"from_zero_to_hero_duration\": group[group.subs > upper_bound][\n",
    "                    \"datetime\"\n",
    "                ].iloc[0]\n",
    "                - group[group.subs < lower_bound][\"datetime\"].iloc[-1]\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    return time_to_reach_X_subs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_10K_to_1M = get_df_channel_ascension(10_000, 1_000_000)\n",
    "print(\n",
    "    \"We have {} channels that begins with 10K subs and reach at least 1M\".format(\n",
    "        channel_10K_to_1M.channel.nunique()\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_channel = channel_10K_to_1M.iloc[0]\n",
    "test = channel_10K_to_1M[channel_10K_to_1M[\"channel\"] == first_channel.channel]\n",
    "# test = test[(test['datetime'] > np.datetime64('2018-07-20')) & (test['datetime'] < np.datetime64('2019-01-20'))]\n",
    "test.plot(x=\"datetime\", y=\"subs\")\n",
    "print(\"There are {} weeks for this channel\".format(test.shape[0]))\n",
    "print(\n",
    "    \"We should have approximately the same value : \\n{} and {}\".format(\n",
    "        7 * test.shape[0], test.iloc[-1].datetime - test.iloc[0].datetime\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the time it takes to go from less than 10K to 1M\n",
    "time_to_reach_1M = get_mean_time_to_reach_X_subs(channel_10K_to_1M, 10_000, 1_000_000)\n",
    "time_to_reach_1M.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"In average YTbers take {} days to reach 1M of subscribers\".format(\n",
    "        time_to_reach_1M[\"from_zero_to_hero_duration\"].mean().days\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be nice to bootstrap this result to see with interval of confidence the time taken by Youtuber to reach 1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = time_to_reach_1M[\"from_zero_to_hero_duration\"].apply(\n",
    "    lambda delta_time: delta_time.days\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "conf_interval_reach_1M = bootstrap(\n",
    "    (data,),\n",
    "    np.mean,\n",
    "    confidence_level=0.95,\n",
    "    random_state=rng,\n",
    ").confidence_interval\n",
    "conf_interval_reach_1M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The 95% interval of confidence, the time taken to reach 1M (from 10K) is [{}, {}]\".format(\n",
    "        conf_interval_reach_1M.low, conf_interval_reach_1M.high\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of views, likes, dislikes per categorie and date uploaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task we analyze and plot the data to get some intuition, and use logistic regression to 'predict' the number of views per category and date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas.head()\n",
    "video_metadatas.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I added this [:] to make a copy of the video_metadatas, so that the following operation wont affect the original video_metadatas\n",
    "videos_with_cat = video_metadatas[:]\n",
    "# here I tried to use the same expression as the replace you used below but it doesnt seem to work...\n",
    "videos_with_cat[\"categories\"] = videos_with_cat[\"categories\"].apply(\n",
    "    lambda x: x.replace(\" & \", \"_\")\n",
    ")\n",
    "# delete the rows with empty categories\n",
    "videos_with_cat.categories.replace(\"\", np.nan, inplace=True)\n",
    "videos_with_cat.dropna(subset=[\"categories\"], inplace=True)\n",
    "videos_with_cat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cat = pd.get_dummies(videos_with_cat.categories)\n",
    "video_metadatas_encoded = videos_with_cat.join(encoded_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas_encoded[\"year\"] = video_metadatas_encoded[\"upload_date\"].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_views = video_metadatas_encoded.groupby([\"year\", \"categories\"]).apply(\n",
    "    lambda x: pd.Series({\"mean_view\": x.view_count.mean()})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_views[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the top most viewed categories every year\n",
    "largest_cat_every_year = (\n",
    "    mean_views.groupby(\"year\")[\"mean_view\"].nlargest(2).droplevel(0)\n",
    ")\n",
    "largest_cat_every_year[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, I suggest the following to determine which type of videos are more popular\n",
    "# now we will determine how many videos are uploaded to YouTube by each channel by year\n",
    "video_count_by_year = (\n",
    "    videos_with_cat.dropna(axis=0).groupby([\"categories\", \"yearNumber\"]).sum()\n",
    ")\n",
    "# NOTE THAT CATEGORIES::MOVIES AND SHOWS HAVE VERY FEW DATA!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_view_count_by_year = video_count_by_year.view_count.unstack().reset_index()\n",
    "video_view_count_by_year.columns.name = None\n",
    "\n",
    "video_like_count_by_year = video_count_by_year.like_count.unstack().reset_index()\n",
    "video_like_count_by_year.columns.name = None\n",
    "\n",
    "video_dislike_count_by_year = video_count_by_year.dislike_count.unstack().reset_index()\n",
    "video_dislike_count_by_year.columns.name = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_with_cat.loc[\n",
    "    videos_with_cat[\"categories\"] == \"Movies\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_view_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_view_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:],\n",
    "        label=video_view_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"video view count\")\n",
    "plt.title(\"video view counts per year for each category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_like_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_like_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:],\n",
    "        label=video_like_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"video likes count\")\n",
    "plt.title(\"video likes counts per year for each category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_dislike_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_dislike_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:],\n",
    "        label=video_dislike_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"video dislikes count\")\n",
    "plt.title(\"video dislikes counts per year for each category\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previous graphs are distorted, we normalize by...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALISATION:\n",
    "## 1. view count - normalise wrt. total video views per year, to see which categories is viewed more every year\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_view_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_view_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:]\n",
    "        / video_view_count_by_year.sum(axis=0)[2:],\n",
    "        label=video_view_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"NORMALISED: video view count\")\n",
    "plt.title(\"NORMALISED: video view counts per year for each category\")\n",
    "plt.show()\n",
    "# view on music videos decreases, whereas on entertainment and gaming increases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. like count - normalise wrt. total likes+dislikes for that category for that year\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_like_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_like_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:]\n",
    "        / (\n",
    "            video_like_count_by_year.loc[\n",
    "                i,\n",
    "            ][2:]\n",
    "            + video_dislike_count_by_year.loc[\n",
    "                i,\n",
    "            ][2:]\n",
    "        ),\n",
    "        label=video_like_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"NORMALISED: video likes count\")\n",
    "plt.title(\"NORMALISED: likes per counts per year for each category\")\n",
    "plt.show()\n",
    "\n",
    "# now it is interesting to see that news_politics and education are consistantly less liked\n",
    "# whereas for nonprofits_activism is becoming more loved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. like count versus view count - normalise wrt. total like counts over view counts for each category for each year\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(video_like_count_by_year.shape[0]):\n",
    "    plt.plot(\n",
    "        video_like_count_by_year.loc[\n",
    "            i,\n",
    "        ][2:]\n",
    "        / (\n",
    "            video_view_count_by_year.loc[\n",
    "                i][2:]\n",
    "        ),\n",
    "        label=video_like_count_by_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"NORMALISED: like over views\")\n",
    "plt.title(\"NORMALISED: video like counts over view counts per category per year\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join channel data and metadata by channel ID\n",
    "metadata_channels = pd.merge(video_metadatas, df_channels[['channel', 'subscribers_cc']],\n",
    "                 left_on=['channel_id'],right_on=['channel'], how='right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_channels[\"view_subscriber_ratio\"]= metadata_channels[\"view_count\"]/metadata_channels[\"subscribers_cc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_channels_select = metadata_channels[metadata_channels[\"categories\"].isin([\"Comedy\",\"Education\",\"Sports\"])]\n",
    "len(metadata_channels_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metdadata_channels_by_year= (\n",
    "    metadata_channels_select.dropna(axis=0).groupby([\"yearNumber\",\"categories\"], as_index=False)[\"view_subscriber_ratio\"].agg([\"sum\",\"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metdadata_channels_by_year = metdadata_channels_by_year.reset_index()\n",
    "metdadata_channels_by_year[\"mean\"] = metdadata_channels_by_year[\"sum\"]/metdadata_channels_by_year[\"count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metdadata_channels_by_year.sort_values([\"categories\", \"yearNumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=metdadata_channels_by_year[metdadata_channels_by_year[\"yearNumber\"]>2010],\n",
    "                                             x =\"yearNumber\", y=\"mean\", hue=\"categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Least Square to predict the number of views given the category and the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_video_metadata_year(year):\n",
    "    return video_metadatas[video_metadatas[\"year\"] == year]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas[\"year\"] = video_metadatas[\"upload_date\"].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_metadatas_2005 = choose_video_metadata_year(2005)\n",
    "video_metadatas_2005.drop([\"upload_date\", \"channel_id\"], axis=1, inplace=True)\n",
    "video_metadatas_2005.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols(\n",
    "    formula=\"view_count ~ duration + C(categories)\", data=video_metadatas_2005\n",
    ")\n",
    "\n",
    "res = model.fit()\n",
    "print(res.summary())\n",
    "\n",
    "# print(res.predict([120, 'Music']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the # of views divided by the # of subs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the time series dataframe to implement this, it would be easier. \n",
    "df_time_series['year'] = df_time_series.datetime.dt.year\n",
    "df_time_series.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalize the big channel (many subs) by dividing the number of delta views by nb of total subscribers\n",
    "df_time_series['views_over_subs'] = df_time_series.delta_views / df_time_series.subs\n",
    "df_time_series.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series[df_time_series.delta_views == 0].category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_over_subs_per_year = (\n",
    "    df_time_series.groupby([\"category\", \"year\"]).sum()\n",
    ")\n",
    "views_over_subs_per_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_over_subs_per_year = views_over_subs_per_year.views_over_subs.unstack().reset_index()\n",
    "views_over_subs_per_year.columns.name = None\n",
    "views_over_subs_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_COLORS = 15\n",
    "cm = plt.get_cmap('gist_rainbow')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_prop_cycle(color=[cm(1.*i/NUM_COLORS) for i in range(NUM_COLORS)])\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 10)\n",
    "for i in np.arange(views_over_subs_per_year.shape[0]):\n",
    "    plt.plot(\n",
    "        views_over_subs_per_year.loc[\n",
    "            i,\n",
    "        ][2:],\n",
    "        label=views_over_subs_per_year.loc[i,][\n",
    "            0:1\n",
    "        ][0],\n",
    "    )\n",
    "    \n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"year\")\n",
    "plt.ylabel(\"video view count divided by number of subs\")\n",
    "plt.title(\"video view counts divided by nb of subs per year for each category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ranked_channels = df_channels[df_channels.subscriber_rank_sb < 100]\n",
    "top_ranked_channels.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = top_ranked_channels[\"join_date\"].apply(lambda d: d.to_pydatetime().year)\n",
    "np.mean(date, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldest_video = df_channels[\"join_date\"].apply(lambda d: d.to_pydatetime().year)\n",
    "oldest_video.nsmallest(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_with_largest_subscribers = df_channels.nlargest(53, \"subscribers_cc\")\n",
    "channels_with_largest_subscribers.sample(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2552cbddfdf66195580a0cc7030164c04fc13f4bfb78773728a78bbf4f04323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
